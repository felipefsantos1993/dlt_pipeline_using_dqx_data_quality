{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "import random\n",
    "import json\n",
    "\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.generator import DQGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.col_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Alice', 'Bob', 'Esther']\n",
    "genders = ['F', 'M', None]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(1, 91):\n",
    "  data.append((\n",
    "    i, random.choice(names) if random.random() > 0.1 else None,\n",
    "    random.randint(18, 60) if random.random() > 0.5 else None,\n",
    "    random.choice(genders)\n",
    "    ))\n",
    "\n",
    "for _ in range(10):\n",
    "  data.append(random.choice(data))\n",
    "\n",
    "spark = SparkSession.builder.appName('StartingDQX').getOrCreate()\n",
    "\n",
    "ws_client = WorkspaceClient()\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'name', 'age', 'gender'])\n",
    "\n",
    "df = df.withColumn('id', when(col('id') % 15 == 0, None).otherwise(col('id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_profile(data_input):\n",
    "  try:\n",
    "    profiler = DQProfiler(ws_client)\n",
    "    summary_stats, profiles = profiler.profile(data_input)\n",
    "    return summary_stats, profiles\n",
    "  except Exception as e:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats, profiles = data_profile(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('stats', json.dumps(summary_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('profile', profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlt import dlt_table, read_stream, expect, pipeline\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Bronze Layer: ingestão de dados brutos\n",
    "@dlt_table(name=\"bronze_orders\")\n",
    "def bronze_orders():\n",
    "    return (\n",
    "        read_stream(\"cloud_files:/mnt/raw/orders\", format=\"json\")\n",
    "    )\n",
    "\n",
    "# Silver Layer: limpeza e validação com DQX\n",
    "@dlt_table(name=\"silver_orders\")\n",
    "@expect(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@expect_or_drop(\"valid_total_amount\", \"total_amount >= 0\")\n",
    "@expect(\"valid_order_date\", \"order_date IS NOT NULL\")\n",
    "def silver_orders():\n",
    "    df = dlt.read(\"bronze_orders\")\n",
    "    return (\n",
    "        df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "    )\n",
    "\n",
    "# Gold Layer: agregação\n",
    "@dlt_table(name=\"gold_sales_summary\")\n",
    "def gold_sales_summary():\n",
    "    df = dlt.read(\"silver_orders\")\n",
    "    return (\n",
    "        df.groupBy(\"order_date\")\n",
    "          .sum(\"total_amount\")\n",
    "          .withColumnRenamed(\"sum(total_amount)\", \"daily_sales\")\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
