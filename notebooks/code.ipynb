{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "import random\n",
    "import json\n",
    "\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.generator import DQGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.labs.dqx.col_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Alice', 'Bob', 'Esther']\n",
    "genders = ['F', 'M', None]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(1, 91):\n",
    "  data.append((\n",
    "    i, random.choice(names) if random.random() > 0.1 else None,\n",
    "    random.randint(18, 60) if random.random() > 0.5 else None,\n",
    "    random.choice(genders)\n",
    "    ))\n",
    "\n",
    "for _ in range(10):\n",
    "  data.append(random.choice(data))\n",
    "\n",
    "spark = SparkSession.builder.appName('StartingDQX').getOrCreate()\n",
    "\n",
    "ws_client = WorkspaceClient()\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'name', 'age', 'gender'])\n",
    "\n",
    "df = df.withColumn('id', when(col('id') % 15 == 0, None).otherwise(col('id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_profile(data_input):\n",
    "  try:\n",
    "    profiler = DQProfiler(ws_client)\n",
    "    summary_stats, profiles = profiler.profile(data_input)\n",
    "    return summary_stats, profiles\n",
    "  except Exception as e:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats, profiles = data_profile(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('stats', json.dumps(summary_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('profile', profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Tabela com os dados brutos\n",
    "@dlt.table(\n",
    "    comment=\"Dados brutos de usuários extraídos de um sistema externo\"\n",
    ")\n",
    "def raw_users():\n",
    "    return spark.read.format(\"json\").load(\"/mnt/raw_data/users/\")\n",
    "\n",
    "# 2. Tabela com regras de qualidade aplicadas\n",
    "@dlt.table(\n",
    "    comment=\"Dados de usuários limpos com validações de qualidade\"\n",
    ")\n",
    "@dlt.expect(\"id_not_null\", \"id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"age_positive\", \"age > 0\")\n",
    "@dlt.expect_or_fail(\"email_format_valid\", \"email LIKE '%@%.%'\")\n",
    "def cleaned_users():\n",
    "    return dlt.read(\"raw_users\")\n",
    "\n",
    "# 3. Tabela de usuários ativos\n",
    "@dlt.table(\n",
    "    comment=\"Usuários ativos com idade entre 18 e 60\"\n",
    ")\n",
    "def active_users():\n",
    "    return dlt.read(\"cleaned_users\").filter((col(\"age\") >= 18) & (col(\"age\") <= 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# Estrutura do projeto\n",
    "project_files = {\n",
    "    \"dlt_project/pipelines/dlt_pipeline.py\": '''from dlt import dlt_table, read_stream, expect, expect_or_drop\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "@dlt_table(name=\"bronze_orders\")\n",
    "def bronze_orders():\n",
    "    return read_stream(\"cloud_files:/mnt/raw/orders\", format=\"json\")\n",
    "\n",
    "@dlt_table(name=\"silver_orders\")\n",
    "@expect(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@expect_or_drop(\"valid_total_amount\", \"total_amount >= 0\")\n",
    "@expect(\"valid_order_date\", \"order_date IS NOT NULL\")\n",
    "def silver_orders():\n",
    "    df = dlt.read(\"bronze_orders\")\n",
    "    return df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "@dlt_table(name=\"gold_sales_summary\")\n",
    "def gold_sales_summary():\n",
    "    df = dlt.read(\"silver_orders\")\n",
    "    return df.groupBy(\"order_date\").sum(\"total_amount\").withColumnRenamed(\"sum(total_amount)\", \"daily_sales\")\n",
    "''',\n",
    "\n",
    "    \"dlt_project/config/table_config.json\": '''{\n",
    "  \"source_path\": \"/mnt/raw/orders\",\n",
    "  \"source_format\": \"json\",\n",
    "  \"expectations\": {\n",
    "    \"order_id\": \"order_id IS NOT NULL\",\n",
    "    \"total_amount\": \"total_amount >= 0\",\n",
    "    \"order_date\": \"order_date IS NOT NULL\"\n",
    "  }\n",
    "}\n",
    "''',\n",
    "\n",
    "    \"dlt_project/README.md\": '''# Delta Live Tables - Pipeline com Qualidade de Dados\n",
    "\n",
    "Este projeto demonstra um pipeline com Delta Live Tables (DLT) utilizando DQX (Data Quality Expectations) para garantir integridade dos dados.\n",
    "\n",
    "## Camadas\n",
    "\n",
    "- **Bronze**: ingestão dos dados brutos\n",
    "- **Silver**: limpeza e validação com DQX\n",
    "- **Gold**: agregações e métricas\n",
    "\n",
    "## Qualidade dos Dados\n",
    "\n",
    "- `order_id` não pode ser nulo\n",
    "- `total_amount` deve ser ≥ 0 (linhas inválidas são descartadas)\n",
    "- `order_date` não pode ser nulo\n",
    "\n",
    "## Execução\n",
    "\n",
    "1. Carregue arquivos JSON em `/mnt/raw/orders`\n",
    "2. Crie o pipeline no Databricks com o notebook `dlt_pipeline.py`\n",
    "3. Execute e monitore os resultados e falhas no UI do DLT\n",
    "'''\n",
    "}\n",
    "\n",
    "# Criar diretórios e arquivos temporários\n",
    "base_path = Path(\"/mnt/data/dlt_project\")\n",
    "for file_path, content in project_files.items():\n",
    "    path = base_path / \"/\".join(file_path.split(\"/\")[2:])\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(content)\n",
    "\n",
    "# Compactar os arquivos em um zip\n",
    "zip_path = Path(\"/mnt/data/dlt_project.zip\")\n",
    "with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "    for file_path in project_files:\n",
    "        zipf.write(base_path / \"/\".join(file_path.split(\"/\")[2:]), arcname=file_path.split(\"/\", 1)[1])\n",
    "\n",
    "zip_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
